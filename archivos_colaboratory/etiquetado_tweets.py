# -*- coding: utf-8 -*-
"""Etiquetado_Tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ByObNxKqgQJV6F1IGebyHXvhNFOim2F6

#DESCARGA CORPUS Y DATOS NECESARIOS
"""

#@title Archivos necesarios
!git clone https://github.com/maryhc/opinion.git

"""# Etiquetado con NLTK"""

#@title Dependencias previas
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk import word_tokenize

"""## Etiquetado de comentarios de TWEETS 

Luego del analisis del texto se procede al etiquetado de los comentarios como primer paso, este el caso de etiquetado de comentarios positivos de indole político. 
Para ello se hace uso de lo siguiente:

* uso del corpus `cess_esp` https://mailman.uib.no/public/corpora/2007-October/005448.html

* el cual usa una convención de etiquetas gramaticales dada por el grupo EAGLES https://www.cs.upc.edu/~nlp/tools/parole-sp.html
"""

nltk.download('cess_esp')
from nltk.corpus import cess_esp as cess
from nltk import UnigramTagger as ut
from nltk import BigramTagger as bt

#@title Entrenamiendo del tagger por unigramas
cess_sents = cess.tagged_sents()
fraction = int(len(cess_sents)*90/100)
uni_tagger = ut(cess_sents[:fraction])
uni_tagger.evaluate(cess_sents[fraction+1:])

file_name_mod = "tweetsn.txt"
mod_file = open(file_name_mod,"w")
raw_file = open('/content/opinion/corpus/tweets.txt').read()

for x in raw_file:
		xf = x.replace("!"," ")
		xf = x.lower()
		mod_file.write(xf)

def remove_non_ascii(words):
    new_words = []
    for word in words:
        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        new_words.append(new_word)
    return new_words

sample = open('/content/tweetsn.txt').read()
words = nltk.word_tokenize(sample)
#words = remove_non_ascii(words)
sample

uni_tagger.tag(words)

#@title Entrenamiento del tagger por bigramas
fraction = int(len(cess_sents)*90/100)
bi_tagger = bt(cess_sents[:fraction])
bi_tagger.evaluate(cess_sents[fraction+1:])

bi_tagger.tag("Yo soy una persona muy amable".split(" "))

"""# Etiquetado mejorado con Stanza (StanfordNLP)

**¿Que es Stanza?**

* El grupo de investigacion en NLP de Stanford tenía una suite de librerias que ejecutaban varias tareas de NLP, esta suite se unifico en un solo servicio que llamaron **CoreNLP** con base en codigo java: https://stanfordnlp.github.io/CoreNLP/index.html

* Para python existe **StanfordNLP**: https://stanfordnlp.github.io/stanfordnlp/index.html

* Sin embargo, **StanfordNLP** ha sido deprecado y las nuevas versiones de la suite de NLP reciben mantenimiento bajo el nombre de **Stanza**: https://stanfordnlp.github.io/stanza/
"""

!pip install stanza

# esta parte puede demorar un poco ....
import stanza
stanza.download('es')

nlp = stanza.Pipeline('es', processors='tokenize,pos')
doc = nlp(sample)

for sentence in doc.sentences:
  for word in sentence.words:
    print(word.text, word.pos)

"""# Referencias adicionales:

* Etiquetado POS con Stanza https://stanfordnlp.github.io/stanza/pos.html#accessing-pos-and-morphological-feature-for-word

* Stanza | Github: https://github.com/stanfordnlp/stanza

* Articulo en ArXiv: https://arxiv.org/pdf/2003.07082.pdf
"""

