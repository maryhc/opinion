# -*- coding: utf-8 -*-
"""RuidoTokenizacion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19kTR00fXeoFpiNO0PNB1q_3Nwm08ptyn
"""

!git clone https://github.com/maryhc/opinion.git
#%cd /content
#!git pull

#https://medium.com/datos-y-ciencia/preprocesamiento-de-datos-de-texto-un-tutorial-en-python-5db5620f1767
import re, string, unicodedata
import nltk
from nltk import FreqDist
nltk.download('punkt')
import matplotlib.pyplot as plt
import numpy as np

!pip install contractions
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer

def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text

def replace_contractions(text):
    return contractions.fix(text)

def limpieza(raw_file,mod_file):
  for x in raw_file:
	    xf = x.lower()
      mod_file.write(xf)

from itertools import count
#file_name = open('/content/opinion/corpus/tweets.txt').read()
#file_name_mod = file_name.replace(".txt","")
file_positivos = "tweetp.txt"
file_negativos = "tweetn.txt"
file_neutros = "tweetnn.txt"
mod_file_p = open(file_positivos,"w")
mod_file_n = open(file_negativos,"w")
mod_file_nn = open(file_neutros,"w")

raw_file_p = open('/content/opinion/corpus/positivo_politica.txt').read()
raw_file_n = open('/content/opinion/corpus/negativo_politica.txt').read()
raw_file_nn = open('/content/opinion/corpus/neutro_politica.txt').read()




	 
limpieza(raw_file_p,mod_file_p)
limpieza(raw_file_n,mod_file_p)
limpieza(raw_file_nn,mod_file_p)

pattern = r'''(?x)
              (?:[A-Z]\.)+
              | \w+(?:-\w+)*
              | \$?\d+(?:\.\d+)?%?
              | \.\.\.
              | [][.,;"'?():-_Â´]
'''

sample = open('/content/tweetsn.txt').read()
def remove_non_ascii(words):
    new_words = []
    for word in words:
        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        new_words.append(new_word)
    return new_words

sample = remove_non_ascii(sample)
#words = nltk.word_tokenize(sample)
words = remove_non_ascii(sample)

vocabulario = sorted(set(words))
print(vocabulario[1000:1050])

#Riqueza lexica
rl = len(vocabulario)/len(words)
print(rl)

def porcentaje_palabra(palabra, texto):
    return 100*texto.count(palabra)/len(texto)

porcentaje_palabra('golpista',words)

#Diccionario
dic = {}
for palabra in set(words):
  dic[palabra] = words.count(palabra)
dic

#FreqDist
fdist = FreqDist(words)
long_words = [palabra for palabra in words if len(palabra)>5]
#vocabulario_filtrado = sorted(set(long_words))
palabras_interesantes = [(palabra, fdist[palabra]) for palabra in set(words) if len(palabra) > 5 and fdist[palabra]>10]
dtypes = [('Palabra','S10'),('Frecuencia',int)]
palabras_interesantes = np.array(palabras_interesantes, dtype = dtypes)
palabras_interesantes

palabras_interesantes = np.sort(palabras_interesantes, order = 'Frecuencia')
palabras_interesantes

top_words = 20
x = np.arange(len(palabras_interesantes[-top_words:]))
y = [freq[1] for freq in palabras_interesantes[-top_words:]]
plt.figure(figsize=(10,5))
plt.plot(x,y)
plt.xticks(x,[str(freq[0]) for freq in palabras_interesantes[-top_words:]], rotation = 'vertical')
plt.grid(True)
plt.show()





def remove_non_ascii(words):
    new_words = []
    for word in words:
        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        new_words.append(new_word)
    return new_words

def to_lowercase(words):
    new_words = []
    for word in words:
        new_word = word.lower()
        new_words.append(new_word)
    return new_words

def remove_punctuation(words):
    new_words = []
    for word in words:
        new_word = re.sub(r'[^\w\s]', '', word)
        if new_word != '':
            new_words.append(new_word)
    return new_words

def replace_numbers(words):
    p = inflect.engine()
    new_words = []
    for word in words:
        if word.isdigit():
            new_word = p.number_to_words(word)
            new_words.append(new_word)
        else:
            new_words.append(word)
    return new_words

def remove_stopwords(words):
    new_words = []
    for word in words:
        if word not in stopwords.words('spanish'):
            new_words.append(word)
    return new_words

def stem_words(words):
    stemmer = LancasterStemmer()
    stems = []
    for word in words:
        stem = stemmer.stem(word)
        stems.append(stem)
    return stems

def lemmatize_verbs(words):
    lemmatizer = WordNetLemmatizer()
    lemmas = []
    for word in words:
        lemma = lemmatizer.lemmatize(word, pos='v')
        lemmas.append(lemma)
    return lemmas

def normalize(words):
    words = remove_non_ascii(words)
    words = to_lowercase(words)
    words = remove_punctuation(words)
    words = replace_numbers(words)
    #words = remove_stopwords(words)
    return words

words = normalize(words)
print(len(words))

print(len(words))

flatten = [w for l in words for w in l]
print(len(flatten))

arr = [w for w in words if re.search('pedofilo',w)]
print(len(arr))